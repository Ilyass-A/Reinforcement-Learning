{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d035cba5",
   "metadata": {},
   "source": [
    "## 1. Installation of stable-baseline3\n",
    "- Stable Baselines3 (SB3) is a set of reliable implementations of reinforcement learning algorithms in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9279516a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stable-baselines3[extra] in c:\\users\\ilyass\\anaconda3\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\ilyass\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (1.2.4)\n",
      "Requirement already satisfied: torch>=1.4.0 in c:\\users\\ilyass\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (1.9.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\ilyass\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (3.3.4)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\ilyass\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (1.6.0)\n",
      "Requirement already satisfied: gym>=0.17 in c:\\users\\ilyass\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (0.19.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\ilyass\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (1.19.5)\n",
      "Requirement already satisfied: atari-py~=0.2.0 in c:\\users\\ilyass\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (0.2.6)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in c:\\users\\ilyass\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (2.6.0)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\ilyass\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (4.5.3.56)\n",
      "Requirement already satisfied: psutil in c:\\users\\ilyass\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (5.8.0)\n",
      "Requirement already satisfied: pillow in c:\\users\\ilyass\\anaconda3\\lib\\site-packages (from stable-baselines3[extra]) (8.2.0)\n",
      "Requirement already satisfied: six in c:\\users\\ilyass\\anaconda3\\lib\\site-packages (from atari-py~=0.2.0->stable-baselines3[extra]) (1.15.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\ilyass\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (2.25.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\ilyass\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.6.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in c:\\users\\ilyass\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.39.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\users\\ilyass\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.35.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in c:\\users\\ilyass\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (3.17.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\ilyass\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.8.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\ilyass\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.4.5)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\users\\ilyass\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (52.0.0.post20210125)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\ilyass\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (1.0.1)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\ilyass\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.36.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\ilyass\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (0.13.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\ilyass\\anaconda3\\lib\\site-packages (from tensorboard>=2.2.0->stable-baselines3[extra]) (3.3.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\ilyass\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (4.7.2)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\users\\ilyass\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\ilyass\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\ilyass\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->stable-baselines3[extra]) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\ilyass\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->stable-baselines3[extra]) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ilyass\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->stable-baselines3[extra]) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\ilyass\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->stable-baselines3[extra]) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ilyass\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->stable-baselines3[extra]) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\ilyass\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->stable-baselines3[extra]) (4.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\ilyass\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->stable-baselines3[extra]) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ilyass\\anaconda3\\lib\\site-packages (from torch>=1.4.0->stable-baselines3[extra]) (3.7.4.3)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\users\\ilyass\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\ilyass\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\users\\ilyass\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\ilyass\\anaconda3\\lib\\site-packages (from matplotlib->stable-baselines3[extra]) (1.3.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\ilyass\\anaconda3\\lib\\site-packages (from pandas->stable-baselines3[extra]) (2021.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a839d4a3",
   "metadata": {},
   "source": [
    "## 2. Import Dependecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48f8db45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # Provides functions for interacting with the operating system (os)\n",
    "import gym # Gym is an open source Python library for developing and comparing reinforcement learning algorithms\n",
    "           # Go to github.com/openai/gym/blob/master/gym/core.py to see the methods a gym env can peform\n",
    "from stable_baselines3 import PPO # Proximal Policy Optimization (PPO) Algorithm\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv # Creates a simple vectorized wrapper for multiple environments \n",
    "from stable_baselines3.common.evaluation import evaluate_policy # Runs policy for n_eval_episodes episodes and returns average reward. This is made to work only with one env."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e04f34",
   "metadata": {},
   "source": [
    "## 3. Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27a3ff55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "environment_name = 'CartPole-v0' # Go to gym.openai.com to discover other availabel environments\n",
    "env = gym.make(environment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef736ec",
   "metadata": {},
   "source": [
    "## 4. Test Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8a2219c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:0\n",
      "Episode:2 Score:0\n",
      "Episode:3 Score:0\n",
      "Episode:4 Score:0\n",
      "Episode:5 Score:0\n"
     ]
    }
   ],
   "source": [
    "episodes = 5 \n",
    "for episode in range(1, episodes+1): \n",
    "    env.reset() # Resets the environment to an initial state and returns an initial observation                        \n",
    "    done = True \n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render() # Gym environment rendering                        \n",
    "        action = env.action_space.sample() # The action_space used in the gym environment is used to define characteristics of the action space of the environment. \n",
    "        n_state, reward, done, info = env.step(action) # Run one timestep of the environment's dynamics, returns: observation, reward, done, information\n",
    "        score += reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc3f82ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ea1aff",
   "metadata": {},
   "source": [
    "## 5. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f0587d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join('Training', 'Logs') # Similar to a logbook, Logs contain all the important records about the course of the event."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa2ae8bc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Logging to Training\\Logs\\PPO_14\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 344  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 5    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 377          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 10           |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060490808 |\n",
      "|    clip_fraction        | 0.0662       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.688       |\n",
      "|    explained_variance   | 0.00712      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.11         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00967     |\n",
      "|    value_loss           | 64.3         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 388         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010387477 |\n",
      "|    clip_fraction        | 0.0739      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.669      |\n",
      "|    explained_variance   | 0.1         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 14.5        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0173     |\n",
      "|    value_loss           | 38.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 402         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008441392 |\n",
      "|    clip_fraction        | 0.0832      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.646      |\n",
      "|    explained_variance   | 0.219       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24.2        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0188     |\n",
      "|    value_loss           | 57.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 407         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006678927 |\n",
      "|    clip_fraction        | 0.053       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.619      |\n",
      "|    explained_variance   | 0.123       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25.9        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0156     |\n",
      "|    value_loss           | 75          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 412         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005498255 |\n",
      "|    clip_fraction        | 0.0459      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.595      |\n",
      "|    explained_variance   | 0.281       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 37.5        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0125     |\n",
      "|    value_loss           | 76.8        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 414        |\n",
      "|    iterations           | 7          |\n",
      "|    time_elapsed         | 34         |\n",
      "|    total_timesteps      | 14336      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00665698 |\n",
      "|    clip_fraction        | 0.0678     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.577     |\n",
      "|    explained_variance   | 0.48       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 37.5       |\n",
      "|    n_updates            | 60         |\n",
      "|    policy_gradient_loss | -0.014     |\n",
      "|    value_loss           | 67.2       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 417          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 39           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049124435 |\n",
      "|    clip_fraction        | 0.0397       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.59        |\n",
      "|    explained_variance   | 0.478        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 18.4         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00736     |\n",
      "|    value_loss           | 63.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 419          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 43           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040083416 |\n",
      "|    clip_fraction        | 0.0257       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.595       |\n",
      "|    explained_variance   | 0.692        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.5         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00277     |\n",
      "|    value_loss           | 52           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 412          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 49           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060791634 |\n",
      "|    clip_fraction        | 0.0479       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.571       |\n",
      "|    explained_variance   | 0.657        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 54.2         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00485     |\n",
      "|    value_loss           | 74.8         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x25dc9e09820>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(environment_name)\n",
    "env = DummyVecEnv([lambda:env])\n",
    "model = PPO('MlpPolicy', env, verbose = 1, tensorboard_log = log_path)\n",
    "model.learn(total_timesteps = 20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607fa7e2",
   "metadata": {},
   "source": [
    "## 6. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbf5b059",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PPO_Path = os.path.join('Training', 'Saved Models', 'PPO_Model')\n",
    "model.save(PPO_Path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785ff82a",
   "metadata": {},
   "source": [
    "## 7. Evaluate and Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5178e80",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ilyass\\anaconda3\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(200.0, 0.0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Frist possibiliy to test the model\n",
    "evaluate_policy(model, env, n_eval_episodes = 10, render = True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebae4b0c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:[200.]\n",
      "Episode:2 Score:[200.]\n",
      "Episode:3 Score:[200.]\n",
      "Episode:4 Score:[200.]\n",
      "Episode:5 Score:[200.]\n"
     ]
    }
   ],
   "source": [
    "# Second possibilty to test the model\n",
    "episodes = 5                             \n",
    "for episode in range(1, episodes+1): \n",
    "    obs = env.reset()\n",
    "    score = 0\n",
    "    \n",
    "    while not done:\n",
    "        env.render()                        \n",
    "        action, _ = model.predict(obs) # Now the model is used here\n",
    "        obs , reward, done, info = env.step(action) \n",
    "        score += reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc7c537",
   "metadata": {},
   "source": [
    "## 8. Viewing Logs in Tensorboard\n",
    "- TensorBoard provides the visualization and tooling needed for machine learning experimentation. For example:\n",
    "  - Tracking and visualizing metrics such as loss and accuracy\n",
    "  - Visualizing the model graph (ops and layers)\n",
    "  - Viewing histograms of weights, biases, or other tensors as they change over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9ddf44e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_log_path = os.path.join(log_path,'PPO_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f4678ea",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "usage: tensorboard [-h] [--helpfull] [--logdir PATH] [--logdir_spec PATH_SPEC]\n",
      "                   [--host ADDR] [--bind_all] [--port PORT]\n",
      "                   [--reuse_port BOOL] [--load_fast {false,auto,true}]\n",
      "                   [--extra_data_server_flags EXTRA_DATA_SERVER_FLAGS]\n",
      "                   [--grpc_creds_type {local,ssl,ssl_dev}]\n",
      "                   [--grpc_data_provider PORT] [--purge_orphaned_data BOOL]\n",
      "                   [--db URI] [--db_import] [--inspect] [--version_tb]\n",
      "                   [--tag TAG] [--event_file PATH] [--path_prefix PATH]\n",
      "                   [--window_title TEXT] [--max_reload_threads COUNT]\n",
      "                   [--reload_interval SECONDS] [--reload_task TYPE]\n",
      "                   [--reload_multifile BOOL]\n",
      "                   [--reload_multifile_inactive_secs SECONDS]\n",
      "                   [--generic_data TYPE]\n",
      "                   [--samples_per_plugin SAMPLES_PER_PLUGIN]\n",
      "                   [--whatif-use-unsafe-custom-prediction YOUR_CUSTOM_PREDICT_FUNCTION.py]\n",
      "                   [--whatif-data-dir PATH]\n",
      "                   {serve,dev} ...\n",
      "tensorboard: error: invalid choice: '{training_log_path}' (choose from 'serve', 'dev')\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir = {training_log_path} --host localhost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623704f3",
   "metadata": {},
   "source": [
    "## 9. The Callback\n",
    "- A callback is a set of functions that will be called at given stages of the training procedure. You can use callbacks to access internal state of the RL model during training. It allows one to do monitoring, auto saving, model manipulation, progress bars, …"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "398156f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf1c22ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join('Training', 'Saved Models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55218d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_callback = StopTrainingOnRewardThreshold(reward_threshold = 200, verbose =1) # Stops the training if the mean reward achieved by the RL model is above a threshold\n",
    "eval_callback = EvalCallback(env,\n",
    "                            callback_on_new_best = stop_callback, \n",
    "                            best_model_save_path = save_path, \n",
    "                            eval_freq = 10000,\n",
    "                            verbose = 1 ) # Evaluate periodically the performance of an agent, using a separate test environment. It will save the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "faecfb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "model = PPO('MlpPolicy', env, verbose = 1, tensorboard_log = log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c0fcdae",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\PPO_15\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 525  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 3    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 408         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 10          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008508554 |\n",
      "|    clip_fraction        | 0.103       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | -0.00189    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.41        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0172     |\n",
      "|    value_loss           | 55.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 347         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009237825 |\n",
      "|    clip_fraction        | 0.0646      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.669      |\n",
      "|    explained_variance   | 0.0738      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15          |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0189     |\n",
      "|    value_loss           | 40          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 325         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007615799 |\n",
      "|    clip_fraction        | 0.0869      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.635      |\n",
      "|    explained_variance   | 0.213       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.9        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0183     |\n",
      "|    value_loss           | 56.4        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=185.80 +/- 20.61\n",
      "Episode length: 185.80 +/- 20.61\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 186          |\n",
      "|    mean_reward          | 186          |\n",
      "| time/                   |              |\n",
      "|    total timesteps      | 10000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070152218 |\n",
      "|    clip_fraction        | 0.0539       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.612       |\n",
      "|    explained_variance   | 0.218        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 26.8         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0134      |\n",
      "|    value_loss           | 65.8         |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 287   |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 35    |\n",
      "|    total_timesteps | 10240 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 286         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 42          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008575684 |\n",
      "|    clip_fraction        | 0.0826      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.596      |\n",
      "|    explained_variance   | 0.502       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25.7        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.017      |\n",
      "|    value_loss           | 58.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 287         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 49          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010783758 |\n",
      "|    clip_fraction        | 0.0869      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.562      |\n",
      "|    explained_variance   | 0.433       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 42.1        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0153     |\n",
      "|    value_loss           | 79.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 285          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 57           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050765667 |\n",
      "|    clip_fraction        | 0.0294       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.569       |\n",
      "|    explained_variance   | 0.246        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 41.4         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00295     |\n",
      "|    value_loss           | 71.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 287         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 64          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007210144 |\n",
      "|    clip_fraction        | 0.0866      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.57       |\n",
      "|    explained_variance   | 0.661       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 45          |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00934    |\n",
      "|    value_loss           | 54.2        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 200          |\n",
      "|    mean_reward          | 200          |\n",
      "| time/                   |              |\n",
      "|    total timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037697703 |\n",
      "|    clip_fraction        | 0.0175       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.555       |\n",
      "|    explained_variance   | 0.142        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 54.8         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00294     |\n",
      "|    value_loss           | 106          |\n",
      "------------------------------------------\n",
      "New best mean reward!\n",
      "Stopping training because the mean reward 200.00  is above the threshold 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x25dac3859a0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps = 20000, callback = eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9b7240",
   "metadata": {},
   "source": [
    "## 10. Changing Policies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d05cd97b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Logging to Training\\Logs\\PPO_16\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 558  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 3    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 345         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015575488 |\n",
      "|    clip_fraction        | 0.242       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.681      |\n",
      "|    explained_variance   | -0.00366    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.56        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0274     |\n",
      "|    value_loss           | 21.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 283         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016858265 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.643      |\n",
      "|    explained_variance   | 0.429       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.9        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0363     |\n",
      "|    value_loss           | 26.7        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 257        |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 31         |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01161166 |\n",
      "|    clip_fraction        | 0.176      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.597     |\n",
      "|    explained_variance   | 0.482      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 13.4       |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0278    |\n",
      "|    value_loss           | 39.8       |\n",
      "----------------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------------\n",
      "| eval/                   |             |\n",
      "|    mean_ep_length       | 200         |\n",
      "|    mean_reward          | 200         |\n",
      "| time/                   |             |\n",
      "|    total timesteps      | 10000       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017395493 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.586      |\n",
      "|    explained_variance   | 0.622       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.3         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0192     |\n",
      "|    value_loss           | 28.5        |\n",
      "-----------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 243   |\n",
      "|    iterations      | 5     |\n",
      "|    time_elapsed    | 42    |\n",
      "|    total_timesteps | 10240 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 243         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 50          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004322276 |\n",
      "|    clip_fraction        | 0.0325      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.557      |\n",
      "|    explained_variance   | 0.454       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.33        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00429    |\n",
      "|    value_loss           | 47.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 242          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 59           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049500125 |\n",
      "|    clip_fraction        | 0.04         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.551       |\n",
      "|    explained_variance   | 0.184        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 25.7         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00342     |\n",
      "|    value_loss           | 54.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 246          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 66           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020187711 |\n",
      "|    clip_fraction        | 0.0362       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.536       |\n",
      "|    explained_variance   | 0.574        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 34.1         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | 0.000326     |\n",
      "|    value_loss           | 62.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 245         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 75          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003889649 |\n",
      "|    clip_fraction        | 0.0272      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.542      |\n",
      "|    explained_variance   | 0.752       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23.9        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.000576   |\n",
      "|    value_loss           | 60.1        |\n",
      "-----------------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=200.00 +/- 0.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 200          |\n",
      "|    mean_reward          | 200          |\n",
      "| time/                   |              |\n",
      "|    total timesteps      | 20000        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036291974 |\n",
      "|    clip_fraction        | 0.0225       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.557       |\n",
      "|    explained_variance   | 0.36         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 44.2         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00138     |\n",
      "|    value_loss           | 97.3         |\n",
      "------------------------------------------\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 237   |\n",
      "|    iterations      | 10    |\n",
      "|    time_elapsed    | 86    |\n",
      "|    total_timesteps | 20480 |\n",
      "------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x25dac38c730>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_arch = [dict(pi = [128, 128, 128, 128], vf = [128, 128, 128, 128])]\n",
    "model = PPO('MlpPolicy', env, verbose = 1, tensorboard_log = log_path, policy_kwargs = {'net_arch': net_arch})\n",
    "model.learn(total_timesteps = 20000, callback = eval_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481c5f6b",
   "metadata": {},
   "source": [
    "## 11. Changing Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "105dd6f7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import DQN # Deep Q Learning (DQN)\n",
    "model = DQN('MlpPolicy', env, verbose = 1, tensorboard_log = log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a9a0df2",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eval_callBack' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-00531e35a9cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotaltimesteps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval_callBack\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'eval_callBack' is not defined"
     ]
    }
   ],
   "source": [
    "model.learn(totaltimesteps = 10000, callback = eval_callBack)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
